{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.values, wine.target, stratify=wine.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142, 13])\n",
      "torch.Size([142])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1.1840e+01, 8.9000e-01, 2.5800e+00, 1.8000e+01, 9.4000e+01, 2.2000e+00,\n",
      "        2.2100e+00, 2.2000e-01, 2.3500e+00, 3.0500e+00, 7.9000e-01, 3.0800e+00,\n",
      "        5.2000e+02]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "train = TensorDataset(X_train, y_train)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "# minibiatch\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 13])\n",
      "torch.Size([16])\n",
      "tensor([[1.2160e+01, 1.6100e+00, 2.3100e+00, 2.2800e+01, 9.0000e+01, 1.7800e+00,\n",
      "         1.6900e+00, 4.3000e-01, 1.5600e+00, 2.4500e+00, 1.3300e+00, 2.2600e+00,\n",
      "         4.9500e+02],\n",
      "        [1.3290e+01, 1.9700e+00, 2.6800e+00, 1.6800e+01, 1.0200e+02, 3.0000e+00,\n",
      "         3.2300e+00, 3.1000e-01, 1.6600e+00, 6.0000e+00, 1.0700e+00, 2.8400e+00,\n",
      "         1.2700e+03],\n",
      "        [1.2290e+01, 2.8300e+00, 2.2200e+00, 1.8000e+01, 8.8000e+01, 2.4500e+00,\n",
      "         2.2500e+00, 2.5000e-01, 1.9900e+00, 2.1500e+00, 1.1500e+00, 3.3000e+00,\n",
      "         2.9000e+02],\n",
      "        [1.3880e+01, 1.8900e+00, 2.5900e+00, 1.5000e+01, 1.0100e+02, 3.2500e+00,\n",
      "         3.5600e+00, 1.7000e-01, 1.7000e+00, 5.4300e+00, 8.8000e-01, 3.5600e+00,\n",
      "         1.0950e+03],\n",
      "        [1.2220e+01, 1.2900e+00, 1.9400e+00, 1.9000e+01, 9.2000e+01, 2.3600e+00,\n",
      "         2.0400e+00, 3.9000e-01, 2.0800e+00, 2.7000e+00, 8.6000e-01, 3.0200e+00,\n",
      "         3.1200e+02],\n",
      "        [1.2670e+01, 9.8000e-01, 2.2400e+00, 1.8000e+01, 9.9000e+01, 2.2000e+00,\n",
      "         1.9400e+00, 3.0000e-01, 1.4600e+00, 2.6200e+00, 1.2300e+00, 3.1600e+00,\n",
      "         4.5000e+02],\n",
      "        [1.4750e+01, 1.7300e+00, 2.3900e+00, 1.1400e+01, 9.1000e+01, 3.1000e+00,\n",
      "         3.6900e+00, 4.3000e-01, 2.8100e+00, 5.4000e+00, 1.2500e+00, 2.7300e+00,\n",
      "         1.1500e+03],\n",
      "        [1.2720e+01, 1.8100e+00, 2.2000e+00, 1.8800e+01, 8.6000e+01, 2.2000e+00,\n",
      "         2.5300e+00, 2.6000e-01, 1.7700e+00, 3.9000e+00, 1.1600e+00, 3.1400e+00,\n",
      "         7.1400e+02],\n",
      "        [1.2000e+01, 1.5100e+00, 2.4200e+00, 2.2000e+01, 8.6000e+01, 1.4500e+00,\n",
      "         1.2500e+00, 5.0000e-01, 1.6300e+00, 3.6000e+00, 1.0500e+00, 2.6500e+00,\n",
      "         4.5000e+02],\n",
      "        [1.2070e+01, 2.1600e+00, 2.1700e+00, 2.1000e+01, 8.5000e+01, 2.6000e+00,\n",
      "         2.6500e+00, 3.7000e-01, 1.3500e+00, 2.7600e+00, 8.6000e-01, 3.2800e+00,\n",
      "         3.7800e+02],\n",
      "        [1.3880e+01, 5.0400e+00, 2.2300e+00, 2.0000e+01, 8.0000e+01, 9.8000e-01,\n",
      "         3.4000e-01, 4.0000e-01, 6.8000e-01, 4.9000e+00, 5.8000e-01, 1.3300e+00,\n",
      "         4.1500e+02],\n",
      "        [1.4390e+01, 1.8700e+00, 2.4500e+00, 1.4600e+01, 9.6000e+01, 2.5000e+00,\n",
      "         2.5200e+00, 3.0000e-01, 1.9800e+00, 5.2500e+00, 1.0200e+00, 3.5800e+00,\n",
      "         1.2900e+03],\n",
      "        [1.3860e+01, 1.5100e+00, 2.6700e+00, 2.5000e+01, 8.6000e+01, 2.9500e+00,\n",
      "         2.8600e+00, 2.1000e-01, 1.8700e+00, 3.3800e+00, 1.3600e+00, 3.1600e+00,\n",
      "         4.1000e+02],\n",
      "        [1.2930e+01, 2.8100e+00, 2.7000e+00, 2.1000e+01, 9.6000e+01, 1.5400e+00,\n",
      "         5.0000e-01, 5.3000e-01, 7.5000e-01, 4.6000e+00, 7.7000e-01, 2.3100e+00,\n",
      "         6.0000e+02],\n",
      "        [1.3560e+01, 1.7300e+00, 2.4600e+00, 2.0500e+01, 1.1600e+02, 2.9600e+00,\n",
      "         2.7800e+00, 2.0000e-01, 2.4500e+00, 6.2500e+00, 9.8000e-01, 3.0300e+00,\n",
      "         1.1200e+03],\n",
      "        [1.4100e+01, 2.0200e+00, 2.4000e+00, 1.8800e+01, 1.0300e+02, 2.7500e+00,\n",
      "         2.9200e+00, 3.2000e-01, 2.3800e+00, 6.2000e+00, 1.0700e+00, 2.7500e+00,\n",
      "         1.0600e+03]])\n",
      "--------\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    print(xb)\n",
    "    print(\"-\"*8)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 96)\n",
    "        self.fc2 = nn.Linear(96, 96)\n",
    "        self.fc3 = nn.Linear(96, 96)\n",
    "        self.fc4 = nn.Linear(96, 96)\n",
    "        self.fc5 = nn.Linear(96, 96)\n",
    "        self.fc6 = nn.Linear(96, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 4.822485536336899\n",
      "100 4.855052590370178\n",
      "150 4.797134816646576\n",
      "200 4.846905499696732\n",
      "250 4.629577130079269\n",
      "300 5.394599825143814\n",
      "350 5.591883987188339\n",
      "400 4.243719905614853\n",
      "450 4.976611107587814\n",
      "500 4.305519491434097\n",
      "550 5.489367514848709\n",
      "600 4.5565899312496185\n",
      "650 4.427371293306351\n",
      "700 5.065387651324272\n",
      "750 3.8414291441440582\n",
      "800 5.139560103416443\n",
      "850 4.542719095945358\n",
      "900 4.043749451637268\n",
      "950 3.752570331096649\n",
      "1000 4.432940661907196\n",
      "1050 3.905428186058998\n",
      "1100 5.21517539024353\n",
      "1150 4.360669031739235\n",
      "1200 4.086588010191917\n",
      "1250 3.609330877661705\n",
      "1300 3.3422208428382874\n",
      "1350 3.51890105009079\n",
      "1400 3.2217068821191788\n",
      "1450 4.798635944724083\n",
      "1500 3.5261503756046295\n",
      "1550 3.316948801279068\n",
      "1600 4.4452371299266815\n",
      "1650 3.855015143752098\n",
      "1700 5.137949675321579\n",
      "1750 3.778383180499077\n",
      "1800 4.3511693328619\n",
      "1850 2.962446630001068\n",
      "1900 3.1771743297576904\n",
      "1950 3.15942145884037\n",
      "2000 4.0534925907850266\n",
      "2050 3.218295633792877\n",
      "2100 2.7191073298454285\n",
      "2150 3.093681752681732\n",
      "2200 3.62271910905838\n",
      "2250 2.820213109254837\n",
      "2300 3.664931207895279\n",
      "2350 3.09072108566761\n",
      "2400 2.4865906164050102\n",
      "2450 2.5491819381713867\n",
      "2500 2.7993225157260895\n",
      "2550 2.5597881451249123\n",
      "2600 3.068324103951454\n",
      "2650 2.794888988137245\n",
      "2700 2.805445298552513\n",
      "2750 2.5004462748765945\n",
      "2800 2.3716007620096207\n",
      "2850 3.855902761220932\n",
      "2900 2.143740698695183\n",
      "2950 2.423935577273369\n",
      "3000 3.3833712115883827\n",
      "3050 3.72771917283535\n",
      "3100 3.0467517003417015\n",
      "3150 3.7012800127267838\n",
      "3200 2.3500857912003994\n",
      "3250 3.3750753104686737\n",
      "3300 2.52938574552536\n",
      "3350 2.5751481652259827\n",
      "3400 1.9449890702962875\n",
      "3450 2.0752850621938705\n",
      "3500 3.012163020670414\n",
      "3550 1.8974705711007118\n",
      "3600 3.119883880019188\n",
      "3650 4.483475059270859\n",
      "3700 3.6240114867687225\n",
      "3750 2.2900462225079536\n",
      "3800 1.9505507573485374\n",
      "3850 2.610693573951721\n",
      "3900 1.8197700455784798\n",
      "3950 2.355727929621935\n",
      "4000 1.7973634377121925\n",
      "4050 2.6950932294130325\n",
      "4100 2.2961959540843964\n",
      "4150 3.8875239491462708\n",
      "4200 3.415502369403839\n",
      "4250 1.626866340637207\n",
      "4300 1.5613349378108978\n",
      "4350 2.251560166478157\n",
      "4400 1.9291948527097702\n",
      "4450 2.6392375864088535\n",
      "4500 1.6616069748997688\n",
      "4550 1.4774005636572838\n",
      "4600 2.779115714132786\n",
      "4650 2.206272631883621\n",
      "4700 3.1404354870319366\n",
      "4750 1.7333858609199524\n",
      "4800 4.588057488203049\n",
      "4850 2.006749041378498\n",
      "4900 3.310617685317993\n",
      "4950 2.2595537304878235\n",
      "5000 2.3966348469257355\n",
      "5050 2.2815394923090935\n",
      "5100 2.7010864689946175\n",
      "5150 1.9687549397349358\n",
      "5200 4.829505085945129\n",
      "5250 1.2620020657777786\n",
      "5300 2.2297739535570145\n",
      "5350 2.630499444901943\n",
      "5400 1.5392930880188942\n",
      "5450 3.621172212064266\n",
      "5500 3.718900740146637\n",
      "5550 2.2213976234197617\n",
      "5600 2.075319697149098\n",
      "5650 1.699713945388794\n",
      "5700 2.896397516131401\n",
      "5750 2.0076171830296516\n",
      "5800 4.945987701416016\n",
      "5850 1.1172857619822025\n",
      "5900 0.9329934790730476\n",
      "5950 1.2643046341836452\n",
      "6000 3.5647607892751694\n",
      "6050 2.4763982743024826\n",
      "6100 2.60548522695899\n",
      "6150 2.520704001188278\n",
      "6200 3.90692600607872\n",
      "6250 1.2123623378574848\n",
      "6300 2.6117057614028454\n",
      "6350 0.9722938761115074\n",
      "6400 2.177596792578697\n",
      "6450 1.1617828328162432\n",
      "6500 1.5333535335958004\n",
      "6550 1.6193546578288078\n",
      "6600 1.1831760369241238\n",
      "6650 2.668440818786621\n",
      "6700 1.3800847418606281\n",
      "6750 1.4149145372211933\n",
      "6800 4.146635204553604\n",
      "6850 1.2086116261780262\n",
      "6900 0.9912660159170628\n",
      "6950 3.3042740672826767\n",
      "7000 0.9063762463629246\n",
      "7050 1.1237250939011574\n",
      "7100 1.1222030464559793\n",
      "7150 1.2601158283650875\n",
      "7200 2.553306197747588\n",
      "7250 0.8717158362269402\n",
      "7300 1.3483377676457167\n",
      "7350 1.674523126333952\n",
      "7400 1.2571296468377113\n",
      "7450 4.300568416714668\n",
      "7500 5.748866498470306\n",
      "7550 2.010254468768835\n",
      "7600 1.035076592117548\n",
      "7650 0.9558817464858294\n",
      "7700 1.1190745197236538\n",
      "7750 1.8397959657013416\n",
      "7800 0.8381224535405636\n",
      "7850 1.5387156046926975\n",
      "7900 0.8249808065593243\n",
      "7950 1.3619501143693924\n",
      "8000 2.4194977954030037\n",
      "8050 0.9745560884475708\n",
      "8100 1.2043407335877419\n",
      "8150 1.8645832724869251\n",
      "8200 1.6506870537996292\n",
      "8250 1.500415025278926\n",
      "8300 5.094362139701843\n",
      "8350 2.0844957157969475\n",
      "8400 1.8565052282065153\n",
      "8450 1.4350933991372585\n",
      "8500 0.8118779398500919\n",
      "8550 3.017881542444229\n",
      "8600 3.212623178958893\n",
      "8650 1.558346739038825\n",
      "8700 0.7665743939578533\n",
      "8750 1.260861054994166\n",
      "8800 0.6352570839226246\n",
      "8850 1.0802917033433914\n",
      "8900 4.03334416821599\n",
      "8950 4.528555057942867\n",
      "9000 0.9879240281879902\n",
      "9050 1.2974279075860977\n",
      "9100 0.5836243676021695\n",
      "9150 1.7032996490597725\n",
      "9200 2.6419750712811947\n",
      "9250 3.1007163673639297\n",
      "9300 3.6750157065689564\n",
      "9350 1.3843167424201965\n",
      "9400 0.7093944195657969\n",
      "9450 0.8021988645195961\n",
      "9500 2.266291167587042\n",
      "9550 0.7057626042515039\n",
      "9600 1.2222485300153494\n",
      "9650 2.3515714704990387\n",
      "9700 1.0793006848543882\n",
      "9750 2.740824420005083\n",
      "9800 0.5774383584503084\n",
      "9850 2.1101605966687202\n",
      "9900 0.5428953357040882\n",
      "9950 0.9119331501424313\n",
      "10000 2.597930647432804\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "#         batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "result = torch.max(model(X_test).data, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.data.numpy() == result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611111111111112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(y_test.data.numpy() == result.numpy()) / len(y_test.data.numpy())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
